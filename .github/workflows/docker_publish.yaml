name: spark-docker

on:
  push:
    branches:
      - master

jobs:
  build-base-image:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        repository: [spark]
        java_version: [8]
        hadoop_version: [3.3.0, 3.2.1, 3.1.4, 2.10.1, 2.9.2, 2.8.5, 2.7.7]
        livy_version: [0.7.0]
    steps:
      - uses: actions/checkout@v1
      - run: echo ${{secrets.DOCKER_PASSWORD}} | docker login -u ${{secrets.DOCKER_USERNAME}} --password-stdin
      - run: docker build -q
          -t ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:base-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
          --build-arg HADOOP_VERSION=${{matrix.hadoop_version}}
          --build-arg LIVY_VERSION=${{matrix.livy_version}}
          --build-arg JAVA_VERSION=${{matrix.java_version}}
          src/base
      - run: docker push ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:base-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
      - run: docker rmi ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:base-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}

  build-spark-image:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        repository: [spark]
        java_version: [8]
        spark_version: [3.0.1, 2.4.7]
        hadoop_version: [3.3.0, 3.2.1, 3.1.4, 2.10.1, 2.9.2, 2.8.5, 2.7.7]
    needs: build-base-image
    steps:
      - uses: actions/checkout@v1
      - run: echo ${{secrets.DOCKER_PASSWORD}} | docker login -u ${{secrets.DOCKER_USERNAME}} --password-stdin
      - run: docker build -q
          -t ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
          --build-arg SPARK_VERSION=${{matrix.spark_version}}
          --build-arg HADOOP_VERSION=${{matrix.hadoop_version}}
          --build-arg JAVA_VERSION=${{matrix.java_version}}
          src/spark
      - run: docker push ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
      - run: docker rmi ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}

  build-pyspark-image:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        repository: [pyspark]
        java_version: [8]
        spark_version: [3.0.1, 2.4.7]
        hadoop_version: [3.3.0, 3.2.1, 3.1.4, 2.10.1, 2.9.2, 2.8.5, 2.7.7]
    needs: build-spark-image
    steps:
      - uses: actions/checkout@v1
      - run: echo ${{secrets.DOCKER_PASSWORD}} | docker login -u ${{secrets.DOCKER_USERNAME}} --password-stdin
      - run: docker build -q
          -t ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
          --build-arg SPARK_VERSION=${{matrix.spark_version}}
          --build-arg HADOOP_VERSION=${{matrix.hadoop_version}}
          --build-arg JAVA_VERSION=${{matrix.java_version}}
          src/pyspark
      - run: docker push ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
      - run: docker rmi ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
