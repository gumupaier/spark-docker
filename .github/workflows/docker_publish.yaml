name: spark-docker

on:
  push:
    branches:
      - master

jobs:
  build-base-image:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        repository: [spark]
        java_version: [8]
        hadoop_version: [3.3.0, 3.2.1, 3.1.4, 2.10.1, 2.9.2, 2.8.5]
        livy_version: [0.7.0]
    steps:
      - uses: actions/checkout@v2
      - uses: actions/setup-python@v2
        with:
          python-version: 3.7
      - run: |
          python -m pip install --upgrade pip
          pip install pyyaml
      - run: |
          cd src/base
          python spark_config_loader_generator.py config.yaml
      - run: echo ${{secrets.DOCKER_PASSWORD}} | docker login -u ${{secrets.DOCKER_USERNAME}} --password-stdin
      - run: docker build -q
          -t ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:base-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
          --build-arg HADOOP_VERSION=${{matrix.hadoop_version}}
          --build-arg LIVY_VERSION=${{matrix.livy_version}}
          --build-arg JAVA_VERSION=${{matrix.java_version}}
          src/base
      - run: docker push ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:base-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
      - run: docker rmi ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:base-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}

  build-spark-image:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        repository: [spark]
        java_version: [8]
        spark_version: [3.0.1, 2.4.7]
        hadoop_version: [3.3.0, 3.2.1, 3.1.4, 2.10.1, 2.9.2, 2.8.5]
    needs: build-base-image
    steps:
      - uses: actions/checkout@v2
      - run: echo ${{secrets.DOCKER_PASSWORD}} | docker login -u ${{secrets.DOCKER_USERNAME}} --password-stdin
      - run: docker build -q
          -t ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
          --build-arg SPARK_VERSION=${{matrix.spark_version}}
          --build-arg HADOOP_VERSION=${{matrix.hadoop_version}}
          --build-arg JAVA_VERSION=${{matrix.java_version}}
          src/spark
      - run: docker push ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
      - run: docker rmi ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}

  build-pyspark-image:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        repository: [pyspark]
        java_version: [8]
        spark_version: [3.0.1, 2.4.7]
        hadoop_version: [3.3.0, 3.2.1, 3.1.4, 2.10.1, 2.9.2, 2.8.5]
    needs: build-spark-image
    steps:
      - uses: actions/checkout@v2
      - run: echo ${{secrets.DOCKER_PASSWORD}} | docker login -u ${{secrets.DOCKER_USERNAME}} --password-stdin
      - run: docker build -q
          -t ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
          --build-arg SPARK_VERSION=${{matrix.spark_version}}
          --build-arg HADOOP_VERSION=${{matrix.hadoop_version}}
          --build-arg JAVA_VERSION=${{matrix.java_version}}
          src/pyspark
      - run: docker push ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
      - run: docker rmi ${{secrets.DOCKER_USERNAME}}/${{matrix.repository}}:${{matrix.spark_version}}-hadoop-${{matrix.hadoop_version}}-java${{matrix.java_version}}
